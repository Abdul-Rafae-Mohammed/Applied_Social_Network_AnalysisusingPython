
# coding: utf-8


import networkx as nx
import pandas as pd
import numpy as np
import pickle


# ---
# 
# ## Part 1 - Random Graph Identification
# 
# For the first part ,We will analyze randomly generated graphs and determine which algorithm created them.

P1_Graphs = pickle.load(open('A4_graphs','rb'))
P1_Graphs


# In[ ]:

def degree_distribution(G):
    degrees = G.degree()
    degree_values = sorted(set(degrees.values()))
    histogram = [list(degrees.values()).count(i)/float(nx.number_of_nodes( G)) for i in degree_values]
    return histogram

degree_distribution(P1_Graphs[2])
nx.average_clustering(P1_Graphs[1])
nx.average_shortest_path_length(P1_Graphs[1])
for G in P1_Graphs:
    print(nx.average_clustering(G), nx.average_shortest_path_length(G), len(degree_distribution(G)))


# <br>
# `P1_Graphs` is a list containing 5 networkx graphs. Each of these graphs were generated by one of three possible algorithms:
# * Preferential Attachment (`'PA'`)
# * Small World with low probability of rewiring (`'SW_L'`)
# * Small World with high probability of rewiring (`'SW_H'`)

# 


def graph_identification():
    methods = []
    for G in P1_Graphs:
        clustering = nx.average_clustering(G)
        shortest_path = nx.average_shortest_path_length(G)
        degree_hist = degree_distribution(G)
        if len(degree_hist)>10:
            methods.append('PA')
        elif clustering < 0.1:
            methods.append('SW_H')
        else:
            methods.append('SW_L')
    return methods
# graph_identification()


# ---
# 
# ## Part 2 - Company Emails
# 
# For the second part , I worked with a company's email network where each node corresponds to a person at the company, 
# and each edge indicates that at least one email has been sent between two people.
# 
# The network also contains the node attributes `Department` and `ManagmentSalary`.
# 
# `Department` indicates the department in the company which the person belongs to, and `ManagmentSalary` indicates whether 
# that person is receiving a managment position salary.

# In[ ]:

G = nx.read_gpickle('email_prediction.txt')

print(nx.info(G))
#G.nodes(data=True)[:10]
#G.nodes(data=True)[0][1]


# ### Part 2A - Salary Prediction
# 
# Using network `G`, We identify the people in the network with missing values for the node attribute `ManagementSalary` and 
# predict whether or not these individuals are receiving a managment position salary.
# 
# To accomplish this,We create a matrix of node features using networkx, train a sklearn Neural Network classifier on nodes that have 
# `ManagementSalary` data, and predict a probability of the node receiving a managment salary for nodes where `ManagementSalary` 
# is missing.
# 
# 
# 
# Predictions are the probability that the corresponding employee is receiving a managment position salary.
# 
# The evaluation metric is the Area Under the ROC Curve (AUC).
# 


from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
def salary_predictions():
    
    # Your Code Here
    def is_management(n):
        mgmtSalary = n[1]['ManagementSalary']
        if mgmtSalary == 0:
            return 0
        elif mgmtSalary == 1:
            return 1
        else:
            return None

    df = pd.DataFrame(index=G.nodes())
    
    # Generating Features
    df['betweeness_centrality'] = pd.Series(nx.betweenness_centrality(G, normalized=True))
    df['page_rank'] = pd.Series(nx.pagerank(G))
    df['degree_centrality'] = pd.Series(G.degree())
    df['is_management'] = pd.Series([is_management(node) for node in G.nodes(data=True)])
    df['closeness_centrality'] = pd.Series(nx.closeness_centrality(G, normalized=True))
    df['degree_centrality'] = pd.Series(nx.degree_centrality(G))
    df['clustering'] = pd.Series(nx.clustering(G))
    
    #Creating a training and test set
    df_train = df[~pd.isnull(df['is_management'])]
    df_test = df[pd.isnull(df['is_management'])]
    features = ['clustering', 'degree_centrality', 'degree_centrality', 'closeness_centrality', 'betweeness_centrality', 'page_rank']
    X_train = df_train[features]
    Y_train = df_train['is_management']
    X_test = df_test[features]
    
    #Scaling the features to improve the performance of classifier
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    
    clf = MLPClassifier(hidden_layer_sizes = [10, 5], alpha = 5,
                       random_state = 0, solver='lbfgs', verbose=0)
    clf.fit(X_train_scaled, Y_train)
    test_proba = clf.predict_proba(X_test_scaled)[:, 1]
    return pd.Series(test_proba,X_test.index)
# prediction = salary_predictions()


# ### Part 2B - New Connections Prediction
# 
# For the last part, We predict future connections between employees of the network. The future connections information has been loaded 
# into the variable `future_connections`. The index is a tuple indicating a pair of nodes that currently do not have a connection, and 
# the `Future Connection` column indicates if an edge between those two nodes will exist in the future, where a value of 1.0 indicates 
# a future connection.

# In[ ]:

future_connections = pd.read_csv('Future_Connections.csv', index_col=0, converters={0: eval})
future_connections.head(10)


# Using network `G` and `future_connections`, identify the edges in `future_connections` with missing values and predict whether 
# or not these edges will have a future connection.
# 
# To accomplish this, We create a matrix of features for the edges found in `future_connections` using networkx, 
# train a sklearn classifier on those edges in `future_connections` that have `Future Connection` data, and 
# predict a probability of the edge being a future connection for those edges in `future_connections` where 
# `Future Connection` is missing.

# 
# Predictions are the probability of the corresponding edge being a future connection.
# 
# The evaluation metric is the Area Under the ROC Curve (AUC).
# 

from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
def new_connections_predictions():
    
    #Generating New Features
    for node in G.nodes():
        G.node[node]['community'] = G.node[node]['Department']
    cn_soundarajan_hopcroft = list(nx.cn_soundarajan_hopcroft(G))
    df_cn_srajan_hcroft = pd.DataFrame(index=[(x[0], x[1]) for x in cn_soundarajan_hopcroft])
    df_cn_srajan_hcroft['cn_srajan_hcroft'] = [x[2] for x in cn_soundarajan_hopcroft]
    df = df.join(df_cn_srajan_hcroft,how='outer')
    df['cn_srajan_hcroft'] = df['cn_srajan_hcroft'].fillna(value=0)
    
    df['resource_allocation_index'] = [x[2] for x in list(nx.resource_allocation_index(G))]
    
    df['jaccard_coefficient'] = [x[2] for x in list(nx.jaccard_coefficient(G))]
        
    preferential_attachment = list(nx.preferential_attachment(G))
    df = pd.DataFrame(index=[(x[0], x[1]) for x in preferential_attachment])
    df['preferential_attachment'] = [x[2] for x in preferential_attachment]
    
    df = future_connections.join(df,how='outer')
    df_train = df[~pd.isnull(df['Fut_Conn'])]
    df_test = df[pd.isnull(df['Fut_Conn'])]
    
    #Creating Test and Train Sets
    features = ['cn_srajan_hcroft', 'preferential_attachment', 'resource_allocation_index', 'jaccard_coefficient']
    X_train = df_train[features]
    Y_train = df_train['Fut_Conn']
    X_test = df_test[features]
    
    #Scaling Features to improve Classifier Performance
    scaler = MinMaxScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    clf = MLPClassifier(hidden_layer_sizes = [10, 5], alpha = 5,
                       random_state = 0, solver='lbfgs', verbose=0)
    clf.fit(X_train_scaled, Y_train)
    test_proba = clf.predict_proba(X_test_scaled)[:, 1]
    predictions = pd.Series(test_proba,X_test.index)
    target = future_connections[pd.isnull(future_connections['Fut_Conn'])]
    target['probability'] = [predictions[x] for x in target.index]
    return target['probability']
# new_connections_predictions()

